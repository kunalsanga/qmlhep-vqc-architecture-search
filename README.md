# ğŸ”¬ QMLHEP â€” VQC Architecture Search

A **Variational Quantum Circuit (VQC) Architecture Search** engine built with [PennyLane](https://pennylane.ai/). Inspired by the **QMLHEP** (Quantum Machine Learning for High Energy Physics) initiative.

This project implements **two search strategies** â€” Random Search and Evolutionary Search â€” and compares them as a research progression, building toward the argument that classical search methods have fundamental limitations that motivate more intelligent approaches.

---

## ğŸ—ºï¸ Research Progression (The Big Picture)

```
Stage 1: Random Search
    â†’ Sample architectures randomly, pick the best score
    â†’ Stochastic, no memory, no refinement

Stage 2: Evolutionary Search  â† current stage
    â†’ Start from a population, mutate the best each generation
    â†’ Slight improvement, but still noisy convergence

Stage 3 (future): LLM / Bayesian / RL-guided Search
    â†’ Use semantic reasoning about architecture structure
    â†’ Sample-efficient, can avoid known-bad patterns
```

> **Core thesis**: Classical search strategies exhibit limited sample efficiency and lack semantic reasoning about architecture structure. This motivates intelligent architecture search for quantum circuits.

---

## ğŸ§  What This Project Does

1. **Randomly generates** VQC architectures â€” varying layers, rotation gates (`RX`, `RY`, `RZ`), and entanglement patterns (`none`, `linear`, `full`).
2. **Builds a PennyLane QNode** for each architecture on the `default.qubit` simulator.
3. **Encodes input** via `RY(x[i])` per qubit. Pads with `0.0` for extra qubits when features < qubits.
4. **Trains** each circuit on `make_moons` using `GradientDescentOptimizer`.
5. **Scores** using a hardware-efficiency cost function:

```
Score = Loss + Î»_depth Ã— depth + Î»_CNOT Ã— CNOT_count
```

6. **Random Search**: samples N architectures independently and picks the best.
7. **Evolutionary Search**: starts with a random population, mutates the best each generation.
8. **Exports** all random-search results to `results.csv`.
9. **Visualises** scatter plots + correlation matrix via `plots.py`.

---

## ğŸ“ Project Structure

```
qmlhep-vqc-architecture-search/
â”‚
â”œâ”€â”€ main.py               # Entry point â€” currently runs evolutionary search
â”œâ”€â”€ search.py             # Random search loop + saves results.csv
â”œâ”€â”€ evolution_search.py   # Evolutionary search loop (population + mutation)
â”œâ”€â”€ evolution.py          # mutate_architecture() â€” single-point mutation
â”œâ”€â”€ architecture.py       # Random architecture sampler
â”œâ”€â”€ circuit_builder.py    # Builds PennyLane QNode from architecture config
â”œâ”€â”€ trainer.py            # Gradient descent training with pennylane.numpy weights
â”œâ”€â”€ evaluator.py          # Computes depth, CNOT count, hardware-efficiency score
â”œâ”€â”€ plots.py              # Scatter plots + correlation matrix from results.csv
â”œâ”€â”€ config.py             # All hyperparameters and search space constants
â”œâ”€â”€ results.csv           # Auto-generated after random search run
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Current Configuration (`config.py`)

| Parameter | Value | Description |
|-----------|-------|-------------|
| `MAX_QUBITS` | **3** | Number of qubits per circuit |
| `MAX_LAYERS` | 4 | Maximum variational layers |
| `ALLOWED_ROTATIONS` | RX, RY, RZ | Rotation gates to sample from |
| `ENTANGLEMENT_PATTERNS` | **none, linear, full** | CNOT entanglement topologies |
| `LAMBDA_DEPTH` | 0.01 | Depth penalty in score |
| `LAMBDA_CNOT` | 0.02 | CNOT penalty in score |
| `TRAIN_STEPS` | 40 | Gradient descent steps per circuit |
| `LEARNING_RATE` | 0.1 | Optimizer learning rate |

---

## âš™ï¸ File-by-File Explanation

### `architecture.py`
```python
generate_architecture() â†’ dict
```
Randomly samples: fixed `n_qubits`, random `n_layers`, random 2-gate rotation block, random entanglement.

---

### `circuit_builder.py`
```python
build_circuit(architecture) â†’ QNode
```
- **Input encoding**: `RY(x[i] if i < len(x) else 0.0, wires=i)` â€” safe 3-qubit padding for 2-feature data
- **Rotation block**: parameterized RX/RY/RZ per qubit per layer
- **Entanglement block**:
  - `"none"` â†’ no CNOT (product state only, limited expressivity)
  - `"linear"` â†’ CNOT chain q0â†’q1â†’q2 (moderate, hardware-friendly)
  - `"full"` â†’ all-to-all CNOTs (max entanglement, highest cost)
- **Output**: `<PauliZ(0)>` âˆˆ [-1, +1]

---

### `trainer.py`
```python
train_architecture(circuit, architecture, X, y) â†’ loss
```
- Uses `pennylane.numpy` with `requires_grad=True` â€” **never plain numpy**
- MSE loss over `make_moons` dataset
- `TRAIN_STEPS` steps of `GradientDescentOptimizer`

---

### `evaluator.py`
```python
evaluate_structure(architecture) â†’ (depth, total_gates, cnot_count)
compute_score(loss, depth, cnot_count) â†’ score
```

**CNOT counts per layer (3 qubits):**

| Entanglement | CNOTs/layer | 3 layers total |
|---|---|---|
| `none` | 0 | 0 |
| `linear` | 2 | 6 |
| `full` | 3 | 9 |

**Score** = `loss + 0.01Ã—depth + 0.02Ã—cnot_count`

---

### `search.py` â€” Random Search
```python
run_search(X, y, iterations=10) â†’ (best, results)
```
- Samples N architectures independently (no memory between iterations)
- Saves all results to `results.csv`
- **Limitation**: purely stochastic â€” good result depends on luck

---

### `evolution.py` â€” Mutation Operator
```python
mutate_architecture(architecture) â†’ new_architecture
```
Picks one mutation at random:
- `"layers"` â†’ randomise `n_layers`
- `"rotation"` â†’ resample 2 rotation gates
- `"entanglement"` â†’ switch entanglement pattern

Uses `copy.deepcopy()` to avoid mutating the original.

---

### `evolution_search.py` â€” Evolutionary Search
```python
run_evolution_search(X, y, population_size=4, generations=4) â†’ best
```

**Algorithm:**
```
1. Generate population_size random architectures and evaluate each
2. For each generation:
   a. Select the best architecture (elitism, size=1)
   b. Mutate best â†’ (population_size - 1) new children
   c. Evaluate all children
   d. Replace population with [best] + children
3. Return final best
```

**Current settings**: `population_size=4`, `generations=4` = 4 + 4Ã—3 = 16 total circuit evaluations.

---

### `plots.py`
```python
plot_results(csv_file="results.csv")
```
- Scatter: **Score vs Circuit Depth**
- Scatter: **Loss vs CNOT Count**
- Printed **Correlation Matrix**

---

## ğŸš€ Setup & Installation

```bash
git clone https://github.com/kunalsanga/qmlhep-vqc-architecture-search.git
cd qmlhep-vqc-architecture-search

python -m venv venv
venv\Scripts\activate          # Windows
# source venv/bin/activate     # macOS/Linux

pip install -r requirements.txt
python main.py
```

---

## ğŸ“Š Observed Results

### Random Search (8 iterations, 2 qubits)
```
Best: n_layers=2, rotation=['RY','RX'], entanglement=linear
Loss: 0.5144 | Depth: 4 | CNOT: 2 | Score: 0.5944
```

### Evolutionary Search (4 population, 4 generations, 3 qubits)
```
Gen 1 â†’ Best score: 0.7367
Gen 2 â†’ Best score: 0.7367   â† stalled
Gen 3 â†’ Best score: 0.7338   â† improved
Gen 4 â†’ Best score: 0.7338   â† stalled again

Final best:
  n_qubits: 3 | n_layers: 2 | rotation: ['RZ','RY'] | entanglement: full
  Loss: 0.5738 | Score: 0.7338
```

---

## ğŸ§  Key Research Insights (Read This First When Revisiting!)

---

### 1ï¸âƒ£ Evolution improved â€” but only slightly

```
0.7367 â†’ 0.7338  (Î” = 0.003)
```

The algorithm refined the architecture across generations, confirming that selection pressure works. But the gain is small because:
- Mutation is **random** â€” no gradient in architecture space
- **No crossover** â€” can't combine good traits from multiple parents
- **No memory** â€” doesn't remember which mutations failed before
- **Elitism of 1** â€” diversity collapses fast

> ğŸ“Œ **Remember**: Small gains from evolution are expected and not a failure â€” they're your evidence that *guided* search (LLM/Bayesian/RL) is the natural next step.

---

### 2ï¸âƒ£ Scores are higher in 3-qubit runs â€” that's expected

```
2-qubit random search best score:  0.5944
3-qubit evolution search best:     0.7338
```

This isn't a regression. 3-qubit circuits have:
- More CNOTs â†’ higher CNOT penalty
- Higher depth â†’ higher depth penalty
- More parameters â†’ harder to train in 40 steps

> ğŸ“Œ **Remember**: Don't compare 2-qubit scores to 3-qubit scores directly. They exist in different cost landscapes.

---

### 3ï¸âƒ£ Linear entanglement = sweet spot (confirmed again)

From random search (2 qubits):
```
linear, 2 layers â†’ loss: 0.514, score: 0.594  â† WINNER
full, 3 layers   â†’ loss: 0.590, score: 0.831
none, 4 layers   â†’ loss: 0.741, score: 0.821
```

`none` = no expressivity. `full` = too expensive. `linear` = balanced.

> ğŸ“Œ **Remember**: This is your core empirical result. Cite it in any proposal.

---

### 4ï¸âƒ£ The two stalling points are structural, not bugs

```
Gen 1 â†’ 0.7367
Gen 2 â†’ 0.7367  â† stall
Gen 3 â†’ 0.7338
Gen 4 â†’ 0.7338  â† stall
```

Stalling means: all mutations of the current best scored worse â†’ best carried forward unchanged. This is a known weakness of (1+Î») elitist evolution with no crossover.

> ğŸ“Œ **Remember**: Stalling is evidence for the thesis that random mutation without memory is sample-inefficient.

---

### 5ï¸âƒ£ Why `pennylane.numpy` â€” the one rule you must never forget

```python
# WRONG â€” crashes with "requires_grad unexpected keyword"
weights = np.random.randn(n_layers, n_qubits, n_rot, requires_grad=True)

# CORRECT
import pennylane.numpy as pnp
weights = pnp.random.normal(size=(n_layers, n_qubits, n_rot), requires_grad=True)
```

PennyLane uses the **parameter-shift rule** to compute gradients. Plain `numpy` arrays are invisible to the autograd engine. `pennylane.numpy` wraps numpy and adds the tracking.

> ğŸ“Œ **Remember**: This was the very first bug we fixed. It will bite anyone new to PennyLane.

---

### 6ï¸âƒ£ The score formula is not arbitrary â€” it mirrors real hardware concerns

```
Score = Loss + 0.01Ã—depth + 0.02Ã—CNOT_count
```

- `Î»_CNOT (0.02) > Î»_depth (0.01)` â€” CNOT gates are noisier than single-qubit gates on NISQ devices
- IBM Quantum and Google both report CNOT error rates ~10Ã— higher than single-qubit error rates
- **Lower score = better real-world viability**, not just better training

> ğŸ“Œ **Remember**: This scoring function is what separates this from a plain VQE/classifier. It's hardware-aware.

---

### 7ï¸âƒ£ Input padding for qubit-feature mismatch

```python
qml.RY(x[i] if i < len(x) else 0.0, wires=i)
```

`make_moons` gives 2 features. 3-qubit circuit has 3 wires. The 3rd qubit is encoded as `RY(0)` = identity â†’ starts in `|0âŸ©` but still participates in entanglement with other qubits.

> ğŸ“Œ **Remember**: For real HEP data (many features), you'd have the opposite problem â€” more features than qubits â€” and would need dimensionality reduction (PCA or amplitude encoding).

---

## ğŸ“Œ Publishable-Level Statement You Can Use

> *"We implement and compare two quantum architecture search strategies â€” random sampling and mutation-based evolutionary search â€” on a hardware-efficiency-aware scoring function. While evolutionary search improves upon random sampling through iterative refinement, it exhibits characteristic stalling behaviour due to the absence of crossover and architectural memory. These limitations highlight the sample inefficiency of classical search heuristics in the discrete quantum architecture space, motivating the development of semantically-guided search methods."*

---

## ğŸ”§ How to Run Each Search Mode

**Random Search** (edit `main.py`):
```python
from search import run_search
best, results = run_search(X, y, iterations=10)
```

**Evolutionary Search** (current default):
```python
from evolution_search import run_evolution_search
best = run_evolution_search(X, y, population_size=4, generations=4)
```

---

## ğŸ”§ How to Extend

| Goal | How |
|---|---|
| Add crossover | In `evolution.py`, add `crossover(arch1, arch2)` that swaps sub-components |
| Increase population | Change `population_size` in `main.py` |
| More qubits | Change `MAX_QUBITS` in `config.py` |
| Real HEP data | Replace `make_moons` in `main.py` with your feature matrix |
| Bayesian search | Replace mutation with a Gaussian process surrogate model over architecture configs |
| Save evolution log | Append each generation's best to a CSV inside `evolution_search.py` |

---

## ğŸ“š References

- [PennyLane Documentation](https://docs.pennylane.ai/)
- [QMLHEP GSoC Project](https://hepsoftwarefoundation.org/gsoc/2024/proposal_QMLHEP.html)
- [Variational Quantum Circuits â€” Schuld et al.](https://arxiv.org/abs/1803.00745)
- [Hardware-Efficient VQE â€” Kandala et al.](https://www.nature.com/articles/nature23879)
- [Parameter-Shift Rule â€” Crooks 2019](https://arxiv.org/abs/1905.13311)
- [Neural Architecture Search Survey â€” Elsken et al.](https://arxiv.org/abs/1808.05377)

---

## ğŸ‘¤ Author

**Kunal Sanga**
GitHub: [@kunalsanga](https://github.com/kunalsanga)
