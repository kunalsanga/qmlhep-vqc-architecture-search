# üî¨ QMLHEP ‚Äî VQC Architecture Search

A lightweight **Variational Quantum Circuit (VQC) Architecture Search** engine built with [PennyLane](https://pennylane.ai/). Inspired by the goals of the **QMLHEP** (Quantum Machine Learning for High Energy Physics) initiative.

The system randomly samples quantum circuit architectures, trains each with gradient descent, scores them using a hardware-efficiency-aware cost function, saves all results to CSV, and generates scatter plots + a correlation matrix for analysis.

---

## üß† What This Project Does

1. **Randomly generates** VQC architectures ‚Äî varying layers, rotation gates (`RX`, `RY`, `RZ`), and entanglement patterns (`none`, `linear`, `full`).
2. **Builds a PennyLane QNode** for each architecture on the `default.qubit` simulator.
3. **Encodes input** via `RY(x[i])` on each qubit. Safely pads with `0.0` for extra qubits when features < qubits.
4. **Trains** each circuit on `make_moons` (binary classification) using `GradientDescentOptimizer`.
5. **Scores** each circuit using a hardware-efficiency-aware cost function:

```
Score = Loss + Œª_depth √ó depth + Œª_CNOT √ó CNOT_count
```

6. **Selects** the best architecture (lowest score) across all iterations.
7. **Exports** all results to `results.csv` for reproducibility and analysis.
8. **Visualises** with scatter plots (Score vs Depth, Loss vs CNOT) and prints a correlation matrix.

---

## üìÅ Project Structure

```
qmlhep-vqc-architecture-search/
‚îÇ
‚îú‚îÄ‚îÄ main.py            # Entry point ‚Äî dataset prep, run search, print best, plot
‚îú‚îÄ‚îÄ search.py          # Full search loop + auto-save to results.csv
‚îú‚îÄ‚îÄ architecture.py    # Random architecture sampler
‚îú‚îÄ‚îÄ circuit_builder.py # Builds PennyLane QNode from architecture config
‚îú‚îÄ‚îÄ trainer.py         # Gradient descent training with pennylane.numpy weights
‚îú‚îÄ‚îÄ evaluator.py       # Computes depth, CNOT count, and hardware-efficiency score
‚îú‚îÄ‚îÄ plots.py           # Loads results.csv ‚Üí scatter plots + correlation matrix
‚îú‚îÄ‚îÄ config.py          # All hyperparameters and search space constants
‚îú‚îÄ‚îÄ results.csv        # Auto-generated after each run
‚îú‚îÄ‚îÄ requirements.txt   # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Current Configuration (`config.py`)

| Parameter | Value | Description |
|-----------|-------|-------------|
| `MAX_QUBITS` | **3** | Number of qubits per circuit |
| `MAX_LAYERS` | 4 | Maximum variational layers to sample |
| `ALLOWED_ROTATIONS` | RX, RY, RZ | Rotation gates |
| `ENTANGLEMENT_PATTERNS` | **none, linear, full** | CNOT topologies |
| `LAMBDA_DEPTH` | 0.01 | Depth penalty in score |
| `LAMBDA_CNOT` | 0.02 | CNOT penalty in score |
| `TRAIN_STEPS` | 40 | Gradient descent steps per circuit |
| `LEARNING_RATE` | 0.1 | Optimizer learning rate |

---

## ‚öôÔ∏è File-by-File Explanation

### `architecture.py`
```python
generate_architecture() ‚Üí dict
```
Randomly samples:
- Fixed `n_qubits = MAX_QUBITS` (currently 3)
- Random `n_layers` ‚àà [1, MAX_LAYERS]
- Random 2-gate rotation block from `ALLOWED_ROTATIONS`
- Random entanglement from `ENTANGLEMENT_PATTERNS`

---

### `circuit_builder.py`
```python
build_circuit(architecture) ‚Üí QNode
```
Builds a PennyLane `@qml.qnode` with:
- **Input encoding**: `RY(x[i])` per qubit. If there are fewer features than qubits, extra wires are encoded with `0.0` (safe padding for 3-qubit circuits on 2-feature datasets).
- **Variational layers**: parameterized RX/RY/RZ rotations per qubit per layer
- **Entanglement block** (per layer):
  - `"none"` ‚Üí no CNOT gates (fast, limited expressivity)
  - `"linear"` ‚Üí CNOT chain: q0‚Üíq1, q1‚Üíq2 (moderate entanglement)
  - `"full"` ‚Üí all-to-all CNOT pairs (maximum entanglement, most expensive)
- **Output**: `<PauliZ(0)>` expectation value ‚àà [-1, +1]

---

### `trainer.py`
```python
train_architecture(circuit, architecture, X, y) ‚Üí loss
```
- Initializes weights with `pennylane.numpy` (`requires_grad=True`) ‚Äî **not plain numpy**
- MSE loss: `Œ£ (prediction - y_i)¬≤ / N`
- Runs `TRAIN_STEPS` steps of `GradientDescentOptimizer`
- Returns final training loss

> ‚ö†Ô∏è **Critical:** You MUST use `pennylane.numpy` (not plain `numpy`) for trainable weights. Plain numpy has no `requires_grad` ‚Äî PennyLane uses this flag to apply the **parameter-shift rule** for quantum gradients.

---

### `evaluator.py`
```python
evaluate_structure(architecture) ‚Üí (depth, total_gates, cnot_count)
compute_score(loss, depth, cnot_count) ‚Üí score
```

**CNOT counts per layer (3 qubits):**
| Entanglement | CNOTs/layer | Total (3 layers) |
|---|---|---|
| `none` | 0 | 0 |
| `linear` | 2 | 6 |
| `full` | 3 | 9 |

**Depth** = `n_layers √ó 2` (one rotation block + one entanglement block per layer)

**Score** = `loss + 0.01√ódepth + 0.02√ócnot_count`

---

### `search.py`
```python
run_search(X, y, iterations=10) ‚Üí (best, results)
```
- Runs the full NAS loop for N iterations
- Saves all results to `results.csv` after completing all iterations
- Returns the architecture with the lowest hardware-efficiency score

---

### `plots.py`
```python
plot_results(csv_file="results.csv")
```
Generates:
1. **Scatter: Score vs Circuit Depth**
2. **Scatter: Loss vs CNOT Count**
3. **Correlation matrix** printed to console

---

### `results.csv` (auto-generated)
| Column | Description |
|--------|-------------|
| `loss` | Final training MSE |
| `depth` | Approximate circuit depth |
| `cnot` | Total CNOT count |
| `score` | Hardware-efficiency score |
| `architecture` | Full architecture config dict |

---

## üöÄ Setup & Installation

### 1. Clone the Repository
```bash
git clone https://github.com/kunalsanga/qmlhep-vqc-architecture-search.git
cd qmlhep-vqc-architecture-search
```

### 2. Create and Activate Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Run
```bash
python main.py
```

---

## üìä Sample Output (3 Qubits, 10 Iterations)

```
===== Iteration 1 =====
Architecture: {'n_qubits': 3, 'n_layers': 3, 'rotation_gates': ['RX', 'RY'], 'entanglement': 'linear'}
Loss: 0.5667
Depth: 6
CNOT: 6
Score: 0.7467

===== Iteration 2 =====
Architecture: {'n_qubits': 3, 'n_layers': 4, 'rotation_gates': ['RY', 'RX'], 'entanglement': 'none'}
Loss: 0.7411
Depth: 8
CNOT: 0
Score: 0.8211

...

===== BEST ARCHITECTURE FOUND =====
n_qubits: 3 | n_layers: 3 | rotation_gates: ['RX', 'RY'] | entanglement: linear
loss: 0.5667 | depth: 6 | cnot: 6 | score: 0.7467

Correlation Matrix:
           loss     depth      cnot     score
loss   1.000000  0.12...   0.14...  0.97...
depth  0.12...   1.000000  0.88...  0.27...
```

---

## üß† Key Research Insights (Important ‚Äî Read Before Revisiting!)

> These are real observations from running this system on 3 qubits across 10+ iterations.

### 1Ô∏è‚É£ Linear entanglement is the sweet spot

```
Best Found: 3 layers + linear entanglement ‚Üí Score: 0.747
```

- `linear` gets good loss (~0.567) without the CNOT overhead of `full`
- `full` gets slightly lower *loss* but higher *score* due to CNOT penalty
- **Remember:** more entanglement ‚â† better score. Balance matters.

### 2Ô∏è‚É£ No entanglement (`none`) limits expressivity

```
none ‚Üí loss always ‚â• 0.74 (even with 4 layers)
```

- Circuits without any CNOT gates can't create entangled states
- Entanglement is what gives quantum circuits their advantage over classical models
- **Remember:** a VQC without entanglement is essentially just a classical neural net with rotations.

### 3Ô∏è‚É£ Full entanglement is not always worth it

```
full, 3 layers ‚Üí loss: 0.590, CNOT: 9, score: 0.831
linear, 3 layers ‚Üí loss: 0.567, CNOT: 6, score: 0.747
```

- `full` uses 50% more CNOTs than `linear` for 3 qubits but gets *worse* overall score
- CNOT gates are the most error-prone on real hardware ‚Üí penalise them
- **Remember:** on NISQ devices, fewer CNOTs = better fidelity. This scoring reflects reality.

### 4Ô∏è‚É£ Why `pennylane.numpy`, not plain `numpy`?

```python
# WRONG - causes TypeError: randn() got unexpected keyword 'requires_grad'
weights = np.random.randn(n_layers, n_qubits, n_rot, requires_grad=True)

# CORRECT
weights = pnp.random.normal(size=(n_layers, n_qubits, n_rot), requires_grad=True)
```

- PennyLane uses the **parameter-shift rule** to compute quantum gradients
- Plain numpy arrays are invisible to PennyLane's autograd engine
- `pennylane.numpy` is a drop-in wrapper that adds gradient tracking
- **Remember:** always use `import pennylane.numpy as pnp` for trainable parameters.

### 5Ô∏è‚É£ The scoring formula is hardware-aware

```
Score = Loss + 0.01√ódepth + 0.02√óCNOT_count
```

- `Œª_CNOT > Œª_depth` because CNOTs are more costly than single-qubit gates on real hardware
- This is inspired by how IBM/Google score circuits for hardware execution
- **Remember:** this is not just a toy metric ‚Äî it's how real quantum ML proposals justify architecture choices.

### 6Ô∏è‚É£ Input encoding for 3 qubits on 2-feature data

```python
# Safe encoding: pad with 0.0 if features < qubits
qml.RY(x[i] if i < len(x) else 0.0, wires=i)
```

- `make_moons` has 2 features, but we have 3 qubits
- The 3rd qubit is initialized in |0‚ü© effectively and still participates in entanglement
- **Remember:** for real HEP data, you'd have many more features and wouldn't need padding.

---

## üìå Publishable-Level Statement

> *"We observe that linear entanglement with moderate circuit depth (3 layers) achieves a favourable trade-off between expressivity and hardware cost on a 3-qubit VQC. Full entanglement reduces training loss marginally but incurs significantly greater CNOT overhead without proportional performance gain. Circuits with no entanglement fail to reach competitive accuracy, confirming that quantum correlations are essential for learning non-linearly separable distributions."*

---

## üîß How to Extend

| Goal | How |
|---|---|
| More qubits | Change `MAX_QUBITS` in `config.py` |
| More layers | Change `MAX_LAYERS` |
| More rotation gates | Add to `ALLOWED_ROTATIONS` |
| Real HEP data | Replace `make_moons` in `main.py` with your feature matrix |
| More iterations | Change `iterations=10` in `main.py` |
| Evolutionary search | Replace random sampling in `architecture.py` with mutation/crossover |
| Better plots | Add Pareto frontier plot in `plots.py` (score vs loss, coloured by entanglement) |

---

## üìö References

- [PennyLane Documentation](https://docs.pennylane.ai/)
- [QMLHEP GSoC Project](https://hepsoftwarefoundation.org/gsoc/2024/proposal_QMLHEP.html)
- [Variational Quantum Circuits ‚Äî Schuld et al.](https://arxiv.org/abs/1803.00745)
- [Hardware-Efficient VQE ‚Äî Kandala et al.](https://www.nature.com/articles/nature23879)
- [Parameter-Shift Rule ‚Äî Crooks 2019](https://arxiv.org/abs/1905.13311)

---

## üë§ Author

**Kunal Sanga**
GitHub: [@kunalsanga](https://github.com/kunalsanga)
